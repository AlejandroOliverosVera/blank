{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejandroOliverosVera/blank/blob/save_state1/FS_iCSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V2h1kRyboiLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6719374-33cb-4283-c618-a4a792e4456f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rnd\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "import sklearn.gaussian_process as gp\n",
        "import tensorflow as tf\n",
        "import time as tm\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "sns.set()\n",
        "DEVICE = 'cpu'"
      ],
      "metadata": {
        "id": "Zvc9s9ssoTFB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ML:\n",
        "  def __init__(self):\n",
        "    # Leer el archivo CSV\n",
        "    self.data = pd.read_csv('/content/drive/MyDrive/data.csv', header = 0)\n",
        "    self.principal_components = self.find_principal_features()\n",
        "\n",
        "  def find_principal_features(self):\n",
        "    pca = PCA()\n",
        "    principal_components = pca.fit_transform(self.data)\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "    # Calculate the cumulative explained variance ratio\n",
        "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "    # Find the number of principal components that explain 95% of the variance\n",
        "    num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "\n",
        "    # Get the loadings of the first num_components\n",
        "    loadings = pca.components_[:num_components]\n",
        "\n",
        "    # Create a binary vector indicating principal features\n",
        "    principal_features = np.zeros(self.data.shape[1])\n",
        "    for loading in loadings:\n",
        "        principal_features[np.abs(loading) >= 0.00001] = 1\n",
        "\n",
        "    return principal_features\n",
        "\n",
        "  def load_data(self, selected_features):\n",
        "    # Usa las caracteristicas dadas por la MH\n",
        "    all_features = list(self.data.columns)\n",
        "    selected_column_names = [all_features[i] for i in range(len(selected_features)) if (selected_features[i] == 1)]\n",
        "    return self.data[selected_column_names].values, self.data.iloc[:, -1].values.astype(float)\n"
      ],
      "metadata": {
        "id": "b4qKir6lokpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ML().principal_components)"
      ],
      "metadata": {
        "id": "v7F5GyYsiBnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLSVM(ML):\n",
        "  def train(self, selected_features):\n",
        "    X, y = self.load_data(selected_features)\n",
        "\n",
        "    # Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    # Normalizar características\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Entrenar un clasificador SVM\n",
        "    svm = SVC()\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Realizar predicciones en el conjunto de prueba\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Calcular las métricas de evaluación\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
        "\n",
        "    return [accuracy, f1]"
      ],
      "metadata": {
        "id": "hJg8JY3fpBof"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crow:\n",
        "  def __init__(self):\n",
        "    self.dimension = 755 ## Depende del DSet\n",
        "    self.position = []\n",
        "    self.metrics = [0,0]\n",
        "    self.pBest = []\n",
        "    self.fBest = 0.0\n",
        "    for j in range(self.dimension):\n",
        "      self.position.append(rnd.randint(0,1))\n",
        "\n",
        "  def countCols(self):\n",
        "    sum = 0\n",
        "    for j in range(self.dimension):\n",
        "        sum += self.position[j]\n",
        "    return sum\n",
        "\n",
        "  def train(self, ml):\n",
        "    self.metrics = ml.train(self.position)\n",
        "\n",
        "  def is_better_than_pbest(self):\n",
        "    if self.fitness() > self.fBest:\n",
        "        self.fBest = self.fitness()\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "  def update_pbest(self):\n",
        "        self.pBest = self.position[:]\n",
        "\n",
        "  def isBetterThan(self, gBest):\n",
        "    return self.fitness() > gBest.fitness()\n",
        "\n",
        "  def move(self, followed_crow: \"Crow\", AP: float, flight_length: float, ml):\n",
        "    for j in range(self.dimension):\n",
        "      if rnd.uniform(0, 1) >= AP:\n",
        "        self.position[j] = int(self.position[j] + rnd.uniform(0, 1) * flight_length * (followed_crow.pBest[j] - self.position[j]))\n",
        "      else:\n",
        "        self.position[j] = rnd.randint(0, 1)\n",
        "    self.position = np.logical_or(self.position, ml.principal_components)\n",
        "    #self.position = [x | y for x, y in zip(self.position, ml.principal_components)]\n",
        "\n",
        "  def fitness(self):\n",
        "    z1 = self.metrics[0] # maximizar f1 score -> [0: malo, 1: bueno]\n",
        "    z2 = self.metrics[1] # maximizar acc -> [0: malo, 1: bueno]\n",
        "    z3 = self.countCols() / self.dimension # minimizar cantidad de columnas que se usan -> [0: bueno, 1: malo]\n",
        "    return z1 * 0.4 + z2 * 0.4 - z3 * 0.2 + 0.2\n",
        "\n",
        "  def copy(self, other: \"Crow\"):\n",
        "    self.position = other.position.copy()\n",
        "    self.fBest = other.fBest\n",
        "\n",
        "  #def toBinary(self, x):\n",
        "    #return 1 if rnd.uniform(0, 1) <= 1 / (1 + math.exp(-x)) else 0\n"
      ],
      "metadata": {
        "id": "LgIdYTn9pN04"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLCSAEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    self.T = 50 #iterMax\n",
        "    self.nCrows = 50 #flock size\n",
        "    self.AP = 0.5\n",
        "    self.flightLenght = 0.75\n",
        "    self.swarm = [] # array of Crow objects\n",
        "    self.gCrow = Crow() #Aquí se guarda el mejor cuervo\n",
        "    self.gCrow_previous = Crow()\n",
        "    self.rnd = rnd\n",
        "    self.sTime = None\n",
        "    self.eTime = None\n",
        "    #self.PC = []\n",
        "    self.observation_space = spaces.Dict({\n",
        "            'agent': spaces.Box(low=0.0, high=2.0, shape=(1,), dtype=np.float32),  ###MI \"AGENTE\" NO ES UN CUERVO, ES EL AP YA QUE ESO ES LO QUE VA VARIANDO PPO\n",
        "            'target': spaces.Box(low=1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "      })\n",
        "\n",
        "    self.action_space = spaces.Discrete(4)\n",
        "    self._action_to_value = {\n",
        "            0: 0,\n",
        "            1: 0.5,\n",
        "            2: -0.5,\n",
        "            3: 1,\n",
        "        }\n",
        "    self.ml = MLSVM()\n",
        "    #self._action_to_direction =\n",
        "\n",
        "  #def execute(self, ml):\n",
        "    #self.startTime()\n",
        "    #self.init(ml)\n",
        "    #self.run(ml)\n",
        "    #self.reset(ml)\n",
        "    #self.step(,ml)\n",
        "    #self.endTime()\n",
        "    #self.log()\n",
        "\n",
        "  def startTime(self):\n",
        "    self.sTime = int(round(tm.time() * 1000))\n",
        "\n",
        "  #def init(self, ml):\n",
        "  #  for i in range(self.nCrows):\n",
        "  #    self.swarm.append(Crow())\n",
        "  #    self.swarm[i].train(ml)\n",
        "  #  self.gCrow.copy(self.swarm[0])\n",
        "  #  for i in range(1, self.nCrows):\n",
        "  #    if self.swarm[i].isBetterThan(self.gCrow):\n",
        "  #      self.gCrow.copy(self.swarm[i])\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "      # We need the following line to seed self.np_random\n",
        "      super().reset(seed=seed)\n",
        "\n",
        "      for i in range(self.nCrows):\n",
        "        self.swarm.append(Crow())\n",
        "        self.swarm[i].train(self.ml)\n",
        "      self.gCrow.copy(self.swarm[0])\n",
        "      for i in range(1, self.nCrows):\n",
        "        if self.swarm[i].isBetterThan(self.gCrow):\n",
        "          self.gCrow.copy(self.swarm[i])\n",
        "\n",
        "      # Choose the agent's location uniformly at random\n",
        "      self._agent_location = 0.5\n",
        "\n",
        "      # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "      self._target_location = 1\n",
        "      #while np.array_equal(self._target_location, self._agent_location):\n",
        "      #    self._target_location = self.np_random.integers(\n",
        "      #        0, self.size, size=2, dtype=int\n",
        "      #    )\n",
        "\n",
        "      observation = self._get_obs()\n",
        "      info = self._get_info()\n",
        "\n",
        "      return observation, info\n",
        "\n",
        "  def run(self):\n",
        "    t = 1\n",
        "    while t <= self.T:\n",
        "      for i in range(self.nCrows):\n",
        "        if self.swarm[i].is_better_than_pbest():\n",
        "            self.swarm[i].update_pbest()\n",
        "        if self.swarm[i].isBetterThan(self.gCrow):\n",
        "            self.gCrow.copy(self.swarm[i])\n",
        "      for i in range(self.nCrows):\n",
        "        followedCrow = self.swarm[self.rnd.randint(0, self.nCrows - 1)]\n",
        "        self.swarm[i].move(followedCrow, self.AP, self.flightLenght, self.ml)\n",
        "        self.swarm[i].train(self.ml)\n",
        "\n",
        "      t += 1\n",
        "\n",
        "  def step(self, action):\n",
        "    direction = self._action_to_value[action]\n",
        "    self._agent_location = self.AP + direction\n",
        "    self.AP = self._agent_location #np.clip(\n",
        "          #self._agent_location + direction, 0, self.size - 1   ####EJECUTAR LA ACCION QUE EN MI CASO ES EL CAMBIO DEL VALOR DEL AP\n",
        "      #)\n",
        "    t = 1\n",
        "    while t <= self.T:\n",
        "      for i in range(self.nCrows):\n",
        "        if self.swarm[i].is_better_than_pbest():\n",
        "            self.swarm[i].update_pbest()\n",
        "        if self.swarm[i].isBetterThan(self.gCrow):\n",
        "            self.gCrow_previous = self.gCrow\n",
        "            self.gCrow.copy(self.swarm[i])\n",
        "      for i in range(self.nCrows):\n",
        "        followedCrow = self.swarm[self.rnd.randint(0, self.nCrows - 1)]\n",
        "        self.swarm[i].move(followedCrow, self.AP, self.flightLenght, self.ml)\n",
        "        self.swarm[i].train(self.ml)\n",
        "\n",
        "      t += 1\n",
        "    terminated = True if self.gCrow.fitness == 1 else 0  #####TERMINA SI EL AGENT ESTA EN EL TARGET, EN ESTE CASO TERMINA SI SE LOGRA EL MAXIMO FITNESS\n",
        "    reward = 1 if self.gCrow.fBest > self.gCrow_previous.fBest else 0  # CALCULAR REWARD, IDEA -> GUARDAR EL BEST FITNESS DE LA STEP ANTERIOR PARA VER SI MEJORÓ O EMPEORÓ\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    return observation, reward, terminated, False, info\n",
        "\n",
        "\n",
        "  def endTime(self):\n",
        "    self.eTime = int(round(tm.time() * 1000))\n",
        "\n",
        "  def log(self):\n",
        "    print(f\"Mejor fitness: {self.gCrow.fBest}, Tiempo total: {self.eTime - self.sTime}ms\")\n",
        "\n",
        "  def _get_obs(self):\n",
        "    return {\"agent\": self.AP, \"target\": self.gCrow.fBest}  #####RETORNAR LA POSICIÓN DEL AGENTE Y LA POSICIÓN DEL TARGET, EN MI CASO EL TARGET DEPENDE DE LA SEPARACION DE LOS CUERVOS\n",
        "\n",
        "  def _get_info(self):\n",
        "    return {\"distance\": 1-self.gCrow.fBest}  #####EN ESTE CASO GET_INFO RETORNA LA DISTANCIA ENTRE EL AGENTE Y EL TARGET\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oWpur0xjY84g"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(RLCSAEnv().ml.principal_components)"
      ],
      "metadata": {
        "id": "fXU8rvfj8cTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy and value model\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "  def __init__(self, obs_space_size, action_space_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.shared_layers = nn.Sequential(\n",
        "        nn.Linear(obs_space_size, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU())\n",
        "\n",
        "    self.policy_layers = nn.Sequential(\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, action_space_size))\n",
        "\n",
        "    self.value_layers = nn.Sequential(\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1))\n",
        "\n",
        "  def value(self, obs):\n",
        "    z = self.shared_layers(obs)\n",
        "    value = self.value_layers(z)\n",
        "    return value\n",
        "\n",
        "  def policy(self, obs):\n",
        "    z = self.shared_layers(obs)\n",
        "    policy_logits = self.policy_layers(z)\n",
        "    return policy_logits\n",
        "\n",
        "  def forward(self, obs):\n",
        "    z = self.shared_layers(obs)\n",
        "    policy_logits = self.policy_layers(z)\n",
        "    value = self.value_layers(z)\n",
        "    return policy_logits, value"
      ],
      "metadata": {
        "id": "HX81Os17cG6x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOTrainer():\n",
        "  def __init__(self,\n",
        "              actor_critic,\n",
        "              ppo_clip_val=0.2,\n",
        "              target_kl_div=0.01,\n",
        "              max_policy_train_iters=80,\n",
        "              value_train_iters=80,\n",
        "              policy_lr=3e-4,\n",
        "              value_lr=1e-2):\n",
        "    self.ac = actor_critic\n",
        "    self.ppo_clip_val = ppo_clip_val\n",
        "    self.target_kl_div = target_kl_div\n",
        "    self.max_policy_train_iters = max_policy_train_iters\n",
        "    self.value_train_iters = value_train_iters\n",
        "\n",
        "    self.n_episodes = 200\n",
        "    self.print_freq = 20\n",
        "\n",
        "    policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "        list(self.ac.policy_layers.parameters())\n",
        "    self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
        "\n",
        "    value_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "        list(self.ac.value_layers.parameters())\n",
        "    self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
        "\n",
        "  def train_policy(self, obs, acts, old_log_probs, gaes):\n",
        "    for _ in range(self.max_policy_train_iters):\n",
        "      self.policy_optim.zero_grad()\n",
        "\n",
        "      new_logits = self.ac.policy(obs)\n",
        "      new_logits = Categorical(logits=new_logits)\n",
        "      new_log_probs = new_logits.log_prob(acts)\n",
        "\n",
        "      policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "      clipped_ratio = policy_ratio.clamp(\n",
        "          1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
        "\n",
        "      clipped_loss = clipped_ratio * gaes\n",
        "      full_loss = policy_ratio * gaes\n",
        "      policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
        "\n",
        "      policy_loss.backward()\n",
        "      self.policy_optim.step()\n",
        "\n",
        "      kl_div = (old_log_probs - new_log_probs).mean()\n",
        "      if kl_div >= self.target_kl_div:\n",
        "        break\n",
        "\n",
        "  def train_value(self, obs, returns):\n",
        "    for _ in range(self.value_train_iters):\n",
        "      self.value_optim.zero_grad()\n",
        "\n",
        "      values = self.ac.value(obs)\n",
        "      value_loss = (returns - values) ** 2\n",
        "      value_loss = value_loss.mean()\n",
        "\n",
        "      value_loss.backward()\n",
        "      self.value_optim.step()\n",
        "\n",
        "  def discount_rewards(self, rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Return discounted rewards based on the given rewards and gamma param.\n",
        "    \"\"\"\n",
        "    new_rewards = [float(rewards[-1])]\n",
        "    for i in reversed(range(len(rewards)-1)):\n",
        "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
        "    return np.array(new_rewards[::-1])\n",
        "\n",
        "  def calculate_gaes(self, rewards, values, gamma=0.99, decay=0.97):\n",
        "    \"\"\"\n",
        "    Return the General Advantage Estimates from the given rewards and values.\n",
        "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
        "    \"\"\"\n",
        "    next_values = np.concatenate([values[1:], [0]])\n",
        "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
        "\n",
        "    gaes = [deltas[-1]]\n",
        "    for i in reversed(range(len(deltas)-1)):\n",
        "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
        "\n",
        "    return np.array(gaes[::-1])\n",
        "\n",
        "  def rollout(self, model, env, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Performs a single rollout.\n",
        "    Returns training data in the shape (n_steps, observation_shape)\n",
        "    and the cumulative reward.\n",
        "    \"\"\"\n",
        "    ### Create data storage\n",
        "    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs\n",
        "    obs = env.reset()\n",
        "\n",
        "    ep_reward = 0\n",
        "    for _ in range(max_steps):\n",
        "        logits, val = model(torch.tensor([obs], dtype=torch.float32,\n",
        "                                         device=DEVICE))\n",
        "        act_distribution = Categorical(logits=logits)\n",
        "        act = act_distribution.sample()\n",
        "        act_log_prob = act_distribution.log_prob(act).item()\n",
        "\n",
        "        act, val = act.item(), val.item()\n",
        "\n",
        "        next_obs, reward, done, _ = env.step(act)\n",
        "\n",
        "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
        "          train_data[i].append(item)\n",
        "\n",
        "        obs = next_obs\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_data = [np.asarray(x) for x in train_data]\n",
        "\n",
        "    ### Do train data filtering\n",
        "    train_data[3] = self.calculate_gaes(train_data[2], train_data[3])\n",
        "\n",
        "    return train_data, ep_reward\n",
        "\n",
        "  def training_loop(self):\n",
        "  # Training loop\n",
        "    ep_rewards = []\n",
        "    for episode_idx in range(self.n_episodes):\n",
        "      # Perform rollout\n",
        "      train_data, reward = self.rollout(model, env)\n",
        "      ep_rewards.append(reward)\n",
        "\n",
        "      # Shuffle\n",
        "      permute_idxs = np.random.permutation(len(train_data[0]))\n",
        "\n",
        "      # Policy data\n",
        "      obs = torch.tensor(train_data[0][permute_idxs],\n",
        "                        dtype=torch.float32, device=DEVICE)\n",
        "      acts = torch.tensor(train_data[1][permute_idxs],\n",
        "                          dtype=torch.int32, device=DEVICE)\n",
        "      gaes = torch.tensor(train_data[3][permute_idxs],\n",
        "                          dtype=torch.float32, device=DEVICE)\n",
        "      act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
        "                                  dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "      # Value data\n",
        "      returns = self.discount_rewards(train_data[2])[permute_idxs]\n",
        "      returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "      # Train model\n",
        "      self.train_policy(obs, acts, act_log_probs, gaes)\n",
        "      self.train_value(obs, returns)\n",
        "\n",
        "      if (episode_idx + 1) % self.print_freq == 0:\n",
        "        print('Episode {} | Avg Reward {:.1f}'.format(\n",
        "            episode_idx + 1, np.mean(ep_rewards[-self.print_freq:])))"
      ],
      "metadata": {
        "id": "wEbAPhhqcLq0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####Codigo para registrar el environment como ambiente de gym\n",
        "\n",
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id='RLCSAEnv-v0',\n",
        "    entry_point='__main__:RLCSAEnv',\n",
        "    max_episode_steps=300,\n",
        ")"
      ],
      "metadata": {
        "id": "Pfnid1OccWz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"RLCSAEnv-v0\")\n",
        "\n",
        "agent_observation_space = env.observation_space['agent']\n",
        "agent_shape = agent_observation_space.shape\n",
        "\n",
        "model = ActorCriticNetwork(agent_shape[0], env.action_space.n)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(\"agent shape = \", agent_shape[0])\n",
        "\n",
        "ppo = PPOTrainer(\n",
        "    model,\n",
        "    policy_lr = 3e-4,\n",
        "    value_lr = 1e-3,\n",
        "    target_kl_div = 0.02,\n",
        "    max_policy_train_iters = 40,\n",
        "    value_train_iters = 40)\n",
        "\n",
        "ml = MLSVM()\n",
        "\n",
        "train_data, reward = ppo.rollout(model, env)\n",
        "ppo.training_loop()"
      ],
      "metadata": {
        "id": "7fgH5ZzPcU2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd5I1-D-F8nV"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "\n",
        "  #CSA().execute(MLSVM()) -> Hay que llamar a PPO\n",
        "except Exception as e:\n",
        "  print(f\"{e} \\nCaused by {e.args}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####EJEMPLO CODIGO PARA CREAR EL GYM ENVIRONMENT\n",
        "'''\n",
        "\n",
        "class RLCSAEnv(gym.Env):\n",
        "    #metadata = {None}\n",
        "\n",
        "  def __init__(self, render_mode=None, size=5):\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),  ###MI \"AGENTE\" NO ES UN CUERVO, ES LA ECUACIÓN QUE EXPLICA LA DISTANCIA ENTRE LOS CUERVOS\n",
        "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int), ###MI TARGET ES EXPLORACION O EXPLOTACIÓN SEGÚN CORRESPONDA\n",
        "            }\n",
        "        )\n",
        "\n",
        "        #NO SE SI TENGO QUE DISCRETIZAR MIS OPCIONES DE ACCION\n",
        "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to\n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "        \"\"\"\n",
        "\n",
        "        #####LAS ACCIONES QUE TENGO SERÍAN MANTENERME EN O CAMBIAR A EXPLORACIÓN Y MANTENERME EN O CAMBIAR A EXPLOTACIÓN\n",
        "        self._action_to_value = {\n",
        "            0: 0,\n",
        "            1: 0.5,\n",
        "            2: 1,\n",
        "            3: 1.5,\n",
        "        }\n",
        "\n",
        "        #assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        #self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        #self.window = None\n",
        "        #self.clock = None\n",
        "\n",
        "  def _get_obs(self):\n",
        "    return {\"agent\": self._agent_location, \"target\": self._target_location}  #####RETORNAR LA POSICIÓN DEL AGENTE Y LA POSICIÓN DEL TARGET, EN MI CASO EL TARGET DEPENDE DE LA SEPARACION DE LOS CUERVOS\n",
        "\n",
        "\n",
        "  def _get_info(self):\n",
        "    return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}  #####EN ESTE CASO GET_INFO RETORNA LA DISTANCIA ENTRE EL AGENTE Y EL TARGET\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "      # We need the following line to seed self.np_random\n",
        "      super().reset(seed=seed)\n",
        "\n",
        "      # Choose the agent's location uniformly at random\n",
        "      self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
        "\n",
        "      # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "      self._target_location = self._agent_location\n",
        "      while np.array_equal(self._target_location, self._agent_location):\n",
        "          self._target_location = self.np_random.integers(\n",
        "              0, self.size, size=2, dtype=int\n",
        "          )\n",
        "\n",
        "      observation = self._get_obs()\n",
        "      info = self._get_info()\n",
        "\n",
        "      return observation, info\n",
        "\n",
        "  def step(self, action):\n",
        "      # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "      direction = self._action_to_value[action] ###FORMATO DE LA PROXIMA ACCION\n",
        "      # We use `np.clip` to make sure we don't leave the grid\n",
        "      self._agent_location = np.clip(\n",
        "          self._agent_location + direction, 0, self.size - 1   ####EJECUTAR LA ACCION QUE EN MI CASO ES EL CAMBIO DEL VALOR DEL AP\n",
        "      )\n",
        "      # An episode is done iff the agent has reached the target\n",
        "      terminated = np.array_equal(self._agent_location, self._target_location)  #####TERMINA SI EL AGENT ESTA EN EL TARGET, EN MI CASO MI TARGET ES FITNESS > FITNESS_PREVIO\n",
        "      reward = 1 if terminated else 0  # Binary sparse rewards\n",
        "      observation = self._get_obs()\n",
        "      info = self._get_info()\n",
        "\n",
        "      if self.render_mode == \"human\":\n",
        "          self._render_frame()\n",
        "\n",
        "      return observation, reward, terminated, False, info\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "kDHrud0CHSaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}