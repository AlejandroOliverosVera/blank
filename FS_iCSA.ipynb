{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejandroOliverosVera/blank/blob/save_state3/FS_iCSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V2h1kRyboiLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac78c13-e88c-446a-9662-e4b290969c29"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rnd\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "import sklearn.gaussian_process as gp\n",
        "import tensorflow as tf\n",
        "import time as tm\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "sns.set()\n",
        "DEVICE = 'cpu'"
      ],
      "metadata": {
        "id": "Zvc9s9ssoTFB"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ML:\n",
        "  def __init__(self):\n",
        "    # Leer el archivo CSV\n",
        "    self.data = pd.read_csv('/content/drive/MyDrive/data.csv', header = 0)\n",
        "    self.principal_components = self.find_principal_features()\n",
        "\n",
        "  def find_principal_features(self):\n",
        "    pca = PCA()\n",
        "    principal_components = pca.fit_transform(self.data)\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "    # Calculate the cumulative explained variance ratio\n",
        "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "    # Find the number of principal components that explain 95% of the variance\n",
        "    num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "\n",
        "    # Get the loadings of the first num_components\n",
        "    loadings = pca.components_[:num_components]\n",
        "\n",
        "    # Create a binary vector indicating principal features\n",
        "    principal_features = np.zeros(self.data.shape[1])\n",
        "    for loading in loadings:\n",
        "        principal_features[np.abs(loading) >= 0.00001] = 1\n",
        "\n",
        "    return principal_features\n",
        "\n",
        "  def load_data(self, selected_features):\n",
        "    # Usa las caracteristicas dadas por la MH\n",
        "    all_features = list(self.data.columns)\n",
        "    selected_column_names = [all_features[i] for i in range(len(selected_features)) if (selected_features[i] == 1)]\n",
        "    return self.data[selected_column_names].values, self.data.iloc[:, -1].values.astype(float)\n"
      ],
      "metadata": {
        "id": "b4qKir6lokpE"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ML().principal_components)"
      ],
      "metadata": {
        "id": "v7F5GyYsiBnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13730b8-8895-40fe-dcad-c82e7703b8b2"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLSVM(ML):\n",
        "  def train(self, selected_features):\n",
        "    X, y = self.load_data(selected_features)\n",
        "\n",
        "    # Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    # Normalizar características\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Entrenar un clasificador SVM\n",
        "    svm = SVC()\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Realizar predicciones en el conjunto de prueba\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Calcular las métricas de evaluación\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
        "\n",
        "    return [accuracy, f1]"
      ],
      "metadata": {
        "id": "hJg8JY3fpBof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6b4e19-c94a-4a08-9255-149b309c16c1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Crow:\n",
        "  def __init__(self):\n",
        "    self.dimension = 755 ## Depende del DSet\n",
        "    self.position = []\n",
        "    self.metrics = [0,0]\n",
        "    self.pBest = []\n",
        "    self.fBest = 0.0\n",
        "    for j in range(self.dimension):\n",
        "      self.position.append(rnd.randint(0,1))\n",
        "\n",
        "  def countCols(self):\n",
        "    sum = 0\n",
        "    for j in range(self.dimension):\n",
        "        sum += self.position[j]\n",
        "    return sum\n",
        "\n",
        "  def train(self, ml):\n",
        "    self.metrics = ml.train(self.position)\n",
        "\n",
        "  def is_better_than_pbest(self):\n",
        "    if self.fitness() > self.fBest:\n",
        "        self.fBest = self.fitness()\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "  def update_pbest(self):\n",
        "        self.pBest = self.position[:]\n",
        "\n",
        "  def isBetterThan(self, gBest):\n",
        "    return self.fitness() > gBest.fBest\n",
        "\n",
        "  def move(self, followed_crow: \"Crow\", AP: float, flight_length: float, ml):\n",
        "    for j in range(self.dimension):\n",
        "      if rnd.uniform(0, 1) >= AP:\n",
        "        self.position[j] = int(self.position[j] + rnd.uniform(0, 1) * flight_length * (int(followed_crow.pBest[j]) - int(self.position[j])))\n",
        "      else:\n",
        "        self.position[j] = rnd.randint(0, 1)\n",
        "    self.position = np.logical_or(self.position, ml.principal_components)\n",
        "\n",
        "  def fitness(self):\n",
        "    z1 = self.metrics[0] # maximizar f1 score -> [0: malo, 1: bueno]\n",
        "    z2 = self.metrics[1] # maximizar acc -> [0: malo, 1: bueno]\n",
        "    z3 = self.countCols() / self.dimension # minimizar cantidad de columnas que se usan -> [0: bueno, 1: malo]\n",
        "    return z1 * 0.4 + z2 * 0.4 - z3 * 0.2 + 0.2\n",
        "\n",
        "  def copy(self, other: \"Crow\"):\n",
        "    self.position = other.position.copy()\n",
        "    self.fBest = other.fBest\n",
        "\n",
        "  #def toBinary(self, x):\n",
        "    #return 1 if rnd.uniform(0, 1) <= 1 / (1 + math.exp(-x)) else 0\n"
      ],
      "metadata": {
        "id": "LgIdYTn9pN04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLCSAEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    self.T = 50 #iterMax\n",
        "    self.nCrows = 50 #flock size\n",
        "    self.AP = 0.5\n",
        "    self.flightLenght = 0.75\n",
        "    self.swarm = [] # array of Crow objects\n",
        "    self.gCrow = Crow() #Aquí se guarda el mejor cuervo\n",
        "    self.gCrow_previous = Crow()\n",
        "    self.rnd = rnd\n",
        "    self.sTime = None\n",
        "    self.eTime = None\n",
        "    high = np.array(\n",
        "            [\n",
        "                0,\n",
        "                np.finfo(np.float32).max,\n",
        "                1,\n",
        "                np.finfo(np.float32).max,\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "    self.observation_space = spaces.Box(high, -high, dtype=np.float32)\n",
        "\n",
        "    self.action_space = spaces.Discrete(4)\n",
        "    self._action_to_value = {\n",
        "            0: 0,\n",
        "            1: 0.33,\n",
        "            2: 0.66,\n",
        "            3: 1,\n",
        "        }\n",
        "    self.ml = MLSVM()\n",
        "    self.state = None\n",
        "\n",
        "  def startTime(self):\n",
        "    self.sTime = int(round(tm.time() * 1000))\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    # We need the following line to seed self.np_random\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    for i in range(self.nCrows):\n",
        "      self.swarm.append(Crow())\n",
        "      self.swarm[i].train(self.ml)\n",
        "    self.gCrow.copy(self.swarm[0])\n",
        "    for i in range(1, self.nCrows):\n",
        "      if self.swarm[i].isBetterThan(self.gCrow):\n",
        "        self.gCrow.copy(self.swarm[i])\n",
        "\n",
        "    self._target_location = 1\n",
        "\n",
        "    self.state = [self.AP, self.gCrow.fBest, 0, 1.0]\n",
        "\n",
        "    #self.AP = self.state[0]\n",
        "    #observation = self._get_obs()\n",
        "    #info = self._get_info()\n",
        "    #obs = np.array(self.state, dtype=np.float32), {}\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    self.AP = self._action_to_value[action]\n",
        "    self.state[0] = self.AP\n",
        "    reward = 0\n",
        "    count = 0\n",
        "    for i in range(self.nCrows):\n",
        "      if self.swarm[i].is_better_than_pbest():\n",
        "          self.swarm[i].update_pbest()\n",
        "          count = count + 1\n",
        "      if self.swarm[i].isBetterThan(self.gCrow):\n",
        "          self.gCrow.copy(self.swarm[i])\n",
        "          print(\"hola\")\n",
        "          reward = 1\n",
        "    for i in range(self.nCrows):\n",
        "      followedCrow = self.swarm[self.rnd.randint(0, self.nCrows - 1)]\n",
        "      self.swarm[i].move(followedCrow, self.AP, self.flightLenght, self.ml)\n",
        "      self.swarm[i].train(self.ml)\n",
        "\n",
        "    self.state = [self.AP, self.gCrow.fBest, count, 1.0]\n",
        "    self.log()\n",
        "\n",
        "    terminated = True\n",
        "    #print(\"STEP: gCrow.fBest = \", self.gCrow.fBest, \"previous: \", self.gCrow_previous.fBest)\n",
        "    #reward = 1 if self.gCrow.fBest > self.gCrow_previous.fBest else 0  # CALCULAR REWARD, IDEA -> GUARDAR EL BEST FITNESS DE LA STEP ANTERIOR PARA VER SI MEJORÓ O EMPEORÓ\n",
        "    observation = self._get_obs()\n",
        "    #info = self._get_info()\n",
        "    print(np.array(self.state, dtype=np.float32), reward)\n",
        "    return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
        "\n",
        "  def endTime(self):\n",
        "    self.eTime = int(round(tm.time() * 1000))\n",
        "\n",
        "  def log(self):\n",
        "    print(f\"Mejor fitness: {self.gCrow.fBest}\")\n",
        "\n",
        "  def _get_obs(self):\n",
        "    #self.state = [self.AP, self.gCrow.fBest, self.gCrow_previous.fBest, 1.0]\n",
        "    return self.state\n",
        "  def _get_info(self):\n",
        "    return {\"distance\": 1-self.gCrow.fBest}  #####EN ESTE CASO GET_INFO RETORNA LA DISTANCIA ENTRE EL AGENTE Y EL TARGET\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oWpur0xjY84g"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy and value model\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "  def __init__(self, obs_space_size, action_space_size):\n",
        "    super().__init__()\n",
        "    self.shared_layers = nn.Sequential(\n",
        "        nn.Linear(obs_space_size, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU())\n",
        "\n",
        "    self.policy_layers = nn.Sequential(\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, action_space_size))\n",
        "\n",
        "    self.value_layers = nn.Sequential(\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1))\n",
        "\n",
        "  def value(self, obs):\n",
        "    z = self.shared_layers(obs)\n",
        "    value = self.value_layers(z)\n",
        "    return value\n",
        "\n",
        "  def policy(self, obs):\n",
        "    z = self.shared_layers(obs)\n",
        "    policy_logits = self.policy_layers(z)\n",
        "    return policy_logits\n",
        "\n",
        "  def forward(self, obs):\n",
        "    z = self.shared_layers(obs)\n",
        "    policy_logits = self.policy_layers(z)\n",
        "    value = self.value_layers(z)\n",
        "    return policy_logits, value"
      ],
      "metadata": {
        "id": "HX81Os17cG6x"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOTrainer():\n",
        "  def __init__(self,\n",
        "              actor_critic,\n",
        "              ppo_clip_val=0.2,\n",
        "              target_kl_div=0.01,\n",
        "              max_policy_train_iters=80,\n",
        "              value_train_iters=80,\n",
        "              policy_lr=3e-4,\n",
        "              value_lr=1e-2):\n",
        "    self.ac = actor_critic\n",
        "    self.ppo_clip_val = ppo_clip_val\n",
        "    self.target_kl_div = target_kl_div\n",
        "    self.max_policy_train_iters = max_policy_train_iters\n",
        "    self.value_train_iters = value_train_iters\n",
        "\n",
        "    self.n_episodes = 20\n",
        "    self.print_freq = 20\n",
        "    self.train_data = []\n",
        "    self.reward = 0\n",
        "    policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "        list(self.ac.policy_layers.parameters())\n",
        "    self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
        "\n",
        "    value_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "        list(self.ac.value_layers.parameters())\n",
        "    self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
        "\n",
        "  def train_policy(self, obs, acts, old_log_probs, gaes):\n",
        "    for _ in range(self.max_policy_train_iters):\n",
        "      self.policy_optim.zero_grad()\n",
        "\n",
        "      new_logits = self.ac.policy(obs)\n",
        "      new_logits = Categorical(logits=new_logits)\n",
        "      new_log_probs = new_logits.log_prob(acts)\n",
        "\n",
        "      policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "      clipped_ratio = policy_ratio.clamp(\n",
        "          1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
        "\n",
        "      clipped_loss = clipped_ratio * gaes\n",
        "      full_loss = policy_ratio * gaes\n",
        "      policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
        "\n",
        "      policy_loss.backward()\n",
        "      self.policy_optim.step()\n",
        "\n",
        "      kl_div = (old_log_probs - new_log_probs).mean()\n",
        "      if kl_div >= self.target_kl_div:\n",
        "        break\n",
        "\n",
        "  def train_value(self, obs, returns):\n",
        "    for _ in range(self.value_train_iters):\n",
        "      self.value_optim.zero_grad()\n",
        "\n",
        "      values = self.ac.value(obs)\n",
        "      value_loss = (returns - values) ** 2\n",
        "      value_loss = value_loss.mean()\n",
        "\n",
        "      value_loss.backward()\n",
        "      self.value_optim.step()\n",
        "\n",
        "  def discount_rewards(self, rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Return discounted rewards based on the given rewards and gamma param.\n",
        "    \"\"\"\n",
        "    new_rewards = [float(rewards[-1])]\n",
        "    for i in reversed(range(len(rewards)-1)):\n",
        "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
        "    return np.array(new_rewards[::-1])\n",
        "\n",
        "  def calculate_gaes(self, rewards, values, gamma=0.99, decay=0.97):\n",
        "    \"\"\"\n",
        "    Return the General Advantage Estimates from the given rewards and values.\n",
        "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
        "    \"\"\"\n",
        "    next_values = np.concatenate([values[1:], [0]])\n",
        "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
        "\n",
        "    gaes = [deltas[-1]]\n",
        "    for i in reversed(range(len(deltas)-1)):\n",
        "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
        "\n",
        "    return np.array(gaes[::-1])\n",
        "\n",
        "  def rollout(self, model, env, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Performs a single rollout.\n",
        "    Returns training data in the shape (n_steps, observation_shape)\n",
        "    and the cumulative reward.\n",
        "    \"\"\"\n",
        "    ### Create data storage\n",
        "    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs\n",
        "    obs = env.reset()\n",
        "    #print(\"obs: \", obs)\n",
        "    #agent_value = obs[0]\n",
        "    #print(\"obs en rollout: \", obs)\n",
        "    ep_reward = 0\n",
        "    for _ in range(max_steps):\n",
        "        logits, val = model(torch.tensor([obs], dtype=torch.float32, device=DEVICE))\n",
        "        act_distribution = Categorical(logits=logits)\n",
        "        act = act_distribution.sample()\n",
        "        act_log_prob = act_distribution.log_prob(act).item()\n",
        "\n",
        "        act, val = act.item(), val.item()\n",
        "        #print(\"act = \", act)\n",
        "        next_obs, reward, done, _ = env.step(act)\n",
        "        #print(\"env.AP = \", env.AP)\n",
        "\n",
        "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
        "          train_data[i].append(item)\n",
        "\n",
        "        obs = next_obs\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_data = [np.asarray(x) for x in train_data]\n",
        "\n",
        "    ### Do train data filtering\n",
        "    train_data[3] = self.calculate_gaes(train_data[2], train_data[3])\n",
        "\n",
        "    return train_data, ep_reward\n",
        "\n",
        "  def training_loop(self, model, env):\n",
        "  # Training loop\n",
        "    ep_rewards = []\n",
        "    for episode_idx in range(self.n_episodes):\n",
        "      # Perform rollout\n",
        "      train_data, reward = self.rollout(model, env)\n",
        "      #train_data = train_data[0]\n",
        "      ep_rewards.append(reward)\n",
        "      #print(\"train_data[0]: \",train_data[0])\n",
        "      # Shuffle\n",
        "      permute_idxs = np.random.permutation(len(train_data[0]))\n",
        "\n",
        "      # Policy data\n",
        "      obs = torch.tensor(train_data[0][permute_idxs],\n",
        "                        dtype=torch.float32, device=DEVICE)\n",
        "      acts = torch.tensor(train_data[1][permute_idxs],\n",
        "                          dtype=torch.int32, device=DEVICE)\n",
        "      gaes = torch.tensor(train_data[3][permute_idxs],\n",
        "                          dtype=torch.float32, device=DEVICE)\n",
        "      act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
        "                                  dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "      # Value data\n",
        "      returns = self.discount_rewards(train_data[2])[permute_idxs]\n",
        "      returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "      # Train model\n",
        "      self.train_policy(obs, acts, act_log_probs, gaes)\n",
        "      self.train_value(obs, returns)\n",
        "\n",
        "#      if (episode_idx + 1) % self.print_freq == 0:\n",
        "#        print('Episode {} | Avg Reward {:.1f}'.format(\n",
        "#            episode_idx + 1, np.mean(ep_rewards[-self.print_freq:])))\n",
        "\n",
        "  def execute(self, model, env):\n",
        "    self.rollout(model, env)\n",
        "    self.training_loop(model, env)\n"
      ],
      "metadata": {
        "id": "wEbAPhhqcLq0"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####Codigo para registrar el environment como ambiente de gym\n",
        "\n",
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id='RLCSAEnv-v0',\n",
        "    entry_point='__main__:RLCSAEnv',\n",
        "    max_episode_steps=300,\n",
        ")"
      ],
      "metadata": {
        "id": "Pfnid1OccWz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"RLCSAEnv-v0\")\n",
        "\n",
        "#agent_observation_space = env.observation_space[0]\n",
        "agent_shape = env.observation_space.shape\n",
        "\n",
        "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "ppo = PPOTrainer(\n",
        "    model,\n",
        "    policy_lr = 3e-4,\n",
        "    value_lr = 1e-3,\n",
        "    target_kl_div = 0.02,\n",
        "    max_policy_train_iters = 40,\n",
        "    value_train_iters = 40)\n",
        "\n",
        "ppo.execute(model, env)\n"
      ],
      "metadata": {
        "id": "7fgH5ZzPcU2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f888894-788d-499b-9d23-21e804b07c37"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:45: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal.\u001b[0m\n",
            "  logger.warn(\"A Box observation space maximum and minimum values are equal.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'list'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  logger.warn(\"Casting input x to numpy array.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola\n",
            "hola\n",
            "hola\n",
            "hola\n",
            "hola\n",
            "hola\n",
            "hola\n",
            "Mejor fitness: 0.8743437378557148\n",
            "[ 0.66        0.87434375 50.          1.        ] 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola\n",
            "hola\n",
            "hola\n",
            "Mejor fitness: 0.8946124769047465\n",
            "[ 0.66       0.8946125 35.         1.       ] 1\n",
            "hola\n",
            "Mejor fitness: 0.9009700927987865\n",
            "[ 1.         0.9009701 19.         1.       ] 1\n",
            "Mejor fitness: 0.9009700927987865\n",
            "[0.        0.9009701 4.        1.       ] 0\n",
            "hola\n",
            "Mejor fitness: 0.9361804106378038\n",
            "[ 0.66       0.9361804 15.         1.       ] 1\n",
            "Mejor fitness: 0.9343261060020422\n",
            "[ 0.         0.9343261 15.         1.       ] 0\n",
            "hola\n",
            "hola\n",
            "Mejor fitness: 0.9669104745147521\n",
            "[ 0.33       0.9669105 12.         1.       ] 1\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.33       0.96452636 7.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[ 0.66        0.96452636 11.          1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.66       0.96452636 2.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[1.         0.96452636 4.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[1.         0.96452636 0.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.66       0.96452636 0.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.66       0.96452636 5.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.         0.96452636 4.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.         0.96452636 7.         1.        ] 0\n",
            "hola\n",
            "hola\n",
            "Mejor fitness: 0.9783478563959569\n",
            "[1.         0.97834784 2.         1.        ] 1\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.33       0.96452636 0.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[1.         0.96452636 8.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.66       0.96452636 1.         1.        ] 0\n",
            "Mejor fitness: 0.9645263685544871\n",
            "[0.66       0.96452636 1.         1.        ] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "yd5I1-D-F8nV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5069a7-ce34-4be5-d545-ac65422183ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPOTrainer.execute() missing 2 required positional arguments: 'model' and 'env' \n",
            "Caused by (\"PPOTrainer.execute() missing 2 required positional arguments: 'model' and 'env'\",)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  ppo.execute()\n",
        "except Exception as e:\n",
        "  print(f\"{e} \\nCaused by {e.args}\")\n"
      ]
    }
  ]
}